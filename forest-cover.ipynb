{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Inspiration: \nhttps://www.kaggle.com/roshanchoudhary/forest-cover-walkthrough-in-python-knn-96-51<br>\nhttps://www.kaggle.com/sharmasanthosh/exploratory-study-on-feature-selection"},{"metadata":{},"cell_type":"markdown","source":"## Data Dictionary\n\n* Elevation = Elevation (altitude) in meters\n* Aspect = Aspect in degrees azimuth\n* Slope = Slope in degrees\n* Horizontal_Distance_To_Hydrology = Horizontal distance to nearest surface water features\n* Vertical_Distance_To_Hydrology = Vertical distance to nearest surface water features\n* Horizontal_Distance_To_Roadways = Horizontal distance to nearest roadway\n* Hillshade_9am = Hill shade index at 9am, summer solstice. Value out of 255\n* Hillshade_Noon = Hill shade index at noon, summer solstice. Value out of 255\n* Hillshade_3pm = Hill shade index at 3pm, summer solstice. Value out of 255\n* Horizontal_Distance_To_Fire_Point = sHorizontal distance to nearest wildfire ignition points\n* Wilderness_Area1 = Rawah Wilderness Area\n* Wilderness_Area2 = Neota Wilderness Area\n* Wilderness_Area3 = Comanche Peak Wilderness Area\n* Wilderness_Area4 = Cache la Poudre Wilderness Area\n* Soil types 1-40\n\n### Tree species:\n1. Spruce/Fir\n2. Lodgepole Pine\n3. Ponderosa Pine\n4. Cottonwood/Willow\n5. Aspen\n6. Douglas-fir\n7. Krummholz"},{"metadata":{},"cell_type":"markdown","source":"### The problem:\nBased on the given data, try to find some patterns in the feature space which influence the result. The result (label) is the forest cover itself, which refers to tree species. With the above features we have to try to find some algorithms that can help us predict the tree species based on environmental/climate parameters on a given plot.\n### Real world application:\nThis prediction can be used for afforestation or reforestation decisions when it comes to finding the right species to plant.\n### Approach\nThe data should be analyzed and statistically described. Then the useful features have to be selected.\nThis is a multi-class classification problem. We have 7 labels and we have to predict which one applies to given parameters.\nTo solve classification problems, we can use machine learning algorithms."},{"metadata":{},"cell_type":"markdown","source":"### Reading the csv files as a dataframe"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/forest-cover-type-dataset/covtype.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking the dimension of the dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There are {} records and {} features in the dataset\".format(df.shape[0], df.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking data types by column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see that all columns contain only integers and there are only non-null values\n### We can also visualize the missing values by column if any:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\nmsno.matrix(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see again, the dataset is very clear, no missing values"},{"metadata":{},"cell_type":"markdown","source":"### We want to see all columns when seeing results, so we need to set this option (*Pandas only shows some columns from the beginning and from the end part of the dataframe by default*)"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Displaying the first 5 rows of the dataframe just to have an overview"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Displaying the main statistical metrics for each column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Some interesting facts:\n* mean elevation is ~2959 m, minimum is 1859 m, maximum is 3858 m\n* mean slope is 14Â°"},{"metadata":{},"cell_type":"markdown","source":"### Take a look at the correlation between variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"sb.set(style=\"white\")\ndf_corr = df.corr()\nmask_train = np.triu(np.ones_like(df_corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(20, 20))\ncmap_train = sb.diverging_palette(220, 10, as_cmap=True)\nsb.heatmap(df_corr, mask=mask_train, cmap=cmap_train, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check skewness"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.skew())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Cover_Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We'll look at skewness visually as well, but first we need to separate the continuous variables from the dataset"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cont_df = df[['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology','Horizontal_Distance_To_Roadways',\n              'Hillshade_9am','Hillshade_Noon','Hillshade_3pm','Horizontal_Distance_To_Fire_Points']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sb.set_style(\"darkgrid\")\nfor i, col in enumerate(cont_df.columns):\n    plt.figure(i)\n    t = sb.distplot(cont_df[col],color=\"g\",label=\"Skewness: {0:.2f}\".format(cont_df[col].skew()))\n    t.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### On the next visuals we can see not just the quartiles and median but also the probability density of the data at different categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Cover_Type']=df['Cover_Type'].astype('category')\n\nfor i, col in enumerate(cont_df.columns):\n    plt.figure(i,figsize=(8,4))\n    sb.violinplot(x=df['Cover_Type'], y=col, data=df, palette=\"mako\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = df.columns\n\n#number of rows=r, number of columns=c\nr,c = df.shape\n\n#Create a new dataframe with r rows, one column for each encoded category, and target\nnewdf = pd.DataFrame(index=np.arange(0, r),columns=['Wilderness_Area','Soil_Type','Cover_Type'])\n\n#Make an entry in 'newdf' for each r as category_id, target value\nfor x in range(0,r):\n    w=0;\n    s=0;\n    \n    # Category1\n    for y in range(10,14):\n        if (df.iloc[x,y] == 1):\n            w=y-9  #category class\n            break\n            \n    # Category2       \n    for z in range(14,54):\n        if (df.iloc[x,z] == 1):\n            s=z-13 #category class\n            break\n    #Make an entry in 'data' for each r as category_id, target value        \n    newdf.iloc[x]=[w,s,df.iloc[x,c-1]]\n\n#Category 1:\nsb.countplot(x=\"Wilderness_Area\", hue=\"Cover_Type\", data=newdf, palette=\"viridis\")\nplt.show()\n\n#Category 2:\nplt.rc(\"figure\", figsize=(25, 10))\nsb.countplot(x=\"Soil_Type\", hue=\"Cover_Type\", data=newdf, palette=\"Spectral\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's look at on the correlations between features again, now only with regards to continuous features"},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nsb.heatmap(cont_df.corr(),cmap='viridis',annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sb.PairGrid(cont_df)\ng.map(sb.scatterplot, color=\"green\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's divide the dataset to features and label"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df.loc[:,'Elevation':'Soil_Type40']\ny=df['Cover_Type']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's remove the features with low variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"colstodrop=['Hillshade_3pm','Soil_Type7','Soil_Type8','Soil_Type14','Soil_Type15',\n     'Soil_Type21','Soil_Type25','Soil_Type28','Soil_Type36','Soil_Type37']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.drop(colstodrop, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split the data into train and test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.0001, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making pipeline including scaling, grid search and cross validation for each model"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"pipe = make_pipeline(StandardScaler(), SVC())  \nparam_grid = {'svc__C': [1e3, 1e4, 1e5],\n              'svc__gamma': [0.001, 0.01, 0.1]}\ngs = GridSearchCV(pipe, param_grid, cv=5)\ngs.fit(X_train[:2000], y_train[:2000])"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"gs.best_params_"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"gs.best_score_"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_svc = make_pipeline(StandardScaler(),SVC(kernel='rbf',C=1000,gamma=0.001))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_svc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf_svc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, y_pred, labels=['Spruce/Fir', 'Lodgepole Pine', 'Ponderosa Pine','Cottonwood/Willow','Aspen','Douglas-fir','Krummholz'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"target_names = ['Spruce/Fir', 'Lodgepole Pine', 'Ponderosa Pine','Cottonwood/Willow','Aspen','Douglas-fir','Krummholz']\nclassification_report(y_test, y_pred, target_names=target_names)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}