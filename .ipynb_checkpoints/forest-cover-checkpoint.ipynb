{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": false,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "#import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    " #       print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspiration: \n",
    "https://www.kaggle.com/roshanchoudhary/forest-cover-walkthrough-in-python-knn-96-51<br>\n",
    "https://www.kaggle.com/sharmasanthosh/exploratory-study-on-feature-selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary\n",
    "\n",
    "* Elevation = Elevation (altitude) in meters\n",
    "* Aspect = Aspect in degrees azimuth\n",
    "* Slope = Slope in degrees\n",
    "* Horizontal_Distance_To_Hydrology = Horizontal distance to nearest surface water features\n",
    "* Vertical_Distance_To_Hydrology = Vertical distance to nearest surface water features\n",
    "* Horizontal_Distance_To_Roadways = Horizontal distance to nearest roadway\n",
    "* Hillshade_9am = Hill shade index at 9am, summer solstice. Value out of 255\n",
    "* Hillshade_Noon = Hill shade index at noon, summer solstice. Value out of 255\n",
    "* Hillshade_3pm = Hill shade index at 3pm, summer solstice. Value out of 255\n",
    "* Horizontal_Distance_To_Fire_Point = sHorizontal distance to nearest wildfire ignition points\n",
    "* Wilderness_Area1 = Rawah Wilderness Area\n",
    "* Wilderness_Area2 = Neota Wilderness Area\n",
    "* Wilderness_Area3 = Comanche Peak Wilderness Area\n",
    "* Wilderness_Area4 = Cache la Poudre Wilderness Area\n",
    "* Soil types 1-40\n",
    "\n",
    "### Tree species:\n",
    "1. Spruce/Fir\n",
    "2. Lodgepole Pine\n",
    "3. Ponderosa Pine\n",
    "4. Cottonwood/Willow\n",
    "5. Aspen\n",
    "6. Douglas-fir\n",
    "7. Krummholz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem:\n",
    "Based on the given data, try to find some patterns in the feature space which influence the result. The result (label) is the forest cover itself, which refers to tree species. With the above features we have to try to find some algorithms that can help us predict the tree species based on environmental/climate parameters on a given plot.\n",
    "### Real world application:\n",
    "This prediction can be used for afforestation or reforestation decisions when it comes to finding the right species to plant.\n",
    "### Approach\n",
    "The data should be analyzed and statistically described. Then the useful features have to be selected.\n",
    "This is a multi-class classification problem. We have 7 labels and we have to predict which one applies to given parameters.\n",
    "To solve classification problems, we can use machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the csv files as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/balintp/Downloads/covtype.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the dimension of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 581012 records and 55 features in the dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} records and {} features in the dataset\".format(df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking data types by column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 581012 entries, 0 to 581011\n",
      "Data columns (total 55 columns):\n",
      " #   Column                              Non-Null Count   Dtype\n",
      "---  ------                              --------------   -----\n",
      " 0   Elevation                           581012 non-null  int64\n",
      " 1   Aspect                              581012 non-null  int64\n",
      " 2   Slope                               581012 non-null  int64\n",
      " 3   Horizontal_Distance_To_Hydrology    581012 non-null  int64\n",
      " 4   Vertical_Distance_To_Hydrology      581012 non-null  int64\n",
      " 5   Horizontal_Distance_To_Roadways     581012 non-null  int64\n",
      " 6   Hillshade_9am                       581012 non-null  int64\n",
      " 7   Hillshade_Noon                      581012 non-null  int64\n",
      " 8   Hillshade_3pm                       581012 non-null  int64\n",
      " 9   Horizontal_Distance_To_Fire_Points  581012 non-null  int64\n",
      " 10  Wilderness_Area1                    581012 non-null  int64\n",
      " 11  Wilderness_Area2                    581012 non-null  int64\n",
      " 12  Wilderness_Area3                    581012 non-null  int64\n",
      " 13  Wilderness_Area4                    581012 non-null  int64\n",
      " 14  Soil_Type1                          581012 non-null  int64\n",
      " 15  Soil_Type2                          581012 non-null  int64\n",
      " 16  Soil_Type3                          581012 non-null  int64\n",
      " 17  Soil_Type4                          581012 non-null  int64\n",
      " 18  Soil_Type5                          581012 non-null  int64\n",
      " 19  Soil_Type6                          581012 non-null  int64\n",
      " 20  Soil_Type7                          581012 non-null  int64\n",
      " 21  Soil_Type8                          581012 non-null  int64\n",
      " 22  Soil_Type9                          581012 non-null  int64\n",
      " 23  Soil_Type10                         581012 non-null  int64\n",
      " 24  Soil_Type11                         581012 non-null  int64\n",
      " 25  Soil_Type12                         581012 non-null  int64\n",
      " 26  Soil_Type13                         581012 non-null  int64\n",
      " 27  Soil_Type14                         581012 non-null  int64\n",
      " 28  Soil_Type15                         581012 non-null  int64\n",
      " 29  Soil_Type16                         581012 non-null  int64\n",
      " 30  Soil_Type17                         581012 non-null  int64\n",
      " 31  Soil_Type18                         581012 non-null  int64\n",
      " 32  Soil_Type19                         581012 non-null  int64\n",
      " 33  Soil_Type20                         581012 non-null  int64\n",
      " 34  Soil_Type21                         581012 non-null  int64\n",
      " 35  Soil_Type22                         581012 non-null  int64\n",
      " 36  Soil_Type23                         581012 non-null  int64\n",
      " 37  Soil_Type24                         581012 non-null  int64\n",
      " 38  Soil_Type25                         581012 non-null  int64\n",
      " 39  Soil_Type26                         581012 non-null  int64\n",
      " 40  Soil_Type27                         581012 non-null  int64\n",
      " 41  Soil_Type28                         581012 non-null  int64\n",
      " 42  Soil_Type29                         581012 non-null  int64\n",
      " 43  Soil_Type30                         581012 non-null  int64\n",
      " 44  Soil_Type31                         581012 non-null  int64\n",
      " 45  Soil_Type32                         581012 non-null  int64\n",
      " 46  Soil_Type33                         581012 non-null  int64\n",
      " 47  Soil_Type34                         581012 non-null  int64\n",
      " 48  Soil_Type35                         581012 non-null  int64\n",
      " 49  Soil_Type36                         581012 non-null  int64\n",
      " 50  Soil_Type37                         581012 non-null  int64\n",
      " 51  Soil_Type38                         581012 non-null  int64\n",
      " 52  Soil_Type39                         581012 non-null  int64\n",
      " 53  Soil_Type40                         581012 non-null  int64\n",
      " 54  Cover_Type                          581012 non-null  int64\n",
      "dtypes: int64(55)\n",
      "memory usage: 243.8 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that all columns contain only integers and there are only non-null values\n",
    "### We can also visualize the missing values by column if any:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Keyring is skipped due to an exception: Failed to unlock the keyring!\u001b[0m\n",
      "Collecting missingno\n",
      "  Downloading missingno-0.4.2-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/lib/python3.9/site-packages (from missingno) (3.3.3)\n",
      "Requirement already satisfied: seaborn in /home/balintp/.local/lib/python3.9/site-packages (from missingno) (0.11.1)\n",
      "Requirement already satisfied: numpy in /usr/lib/python3.9/site-packages (from missingno) (1.19.5)\n",
      "Requirement already satisfied: scipy in /usr/lib/python3.9/site-packages (from missingno) (1.6.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3.9/site-packages (from matplotlib->missingno) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/lib/python3.9/site-packages (from matplotlib->missingno) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3.9/site-packages (from matplotlib->missingno) (8.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/lib/python3.9/site-packages (from matplotlib->missingno) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/lib/python3.9/site-packages (from matplotlib->missingno) (2.8.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/lib/python3.9/site-packages (from seaborn->missingno) (1.2.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3.9/site-packages (from cycler>=0.10->matplotlib->missingno) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3.9/site-packages (from pandas>=0.23->seaborn->missingno) (2020.5)\n",
      "Installing collected packages: missingno\n",
      "Successfully installed missingno-0.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'missingno'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f159fe30545d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmissingno\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmsno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmsno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'missingno'"
     ]
    }
   ],
   "source": [
    "import missingno as msno\n",
    "msno.matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see again, the dataset is very clear, no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to see all columns when seeing results, so we need to set this option (*Pandas only shows some columns from the beginning and from the end part of the dataframe by default*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the first 5 rows of the dataframe just to have an overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the main statistical metrics for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some interesting facts:\n",
    "* mean elevation is ~2959 m, minimum is 1859 m, maximum is 3858 m\n",
    "* mean slope is 14°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at the correlation between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.set(style=\"white\")\n",
    "df_corr = df.corr()\n",
    "mask_train = np.triu(np.ones_like(df_corr, dtype=np.bool))\n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "cmap_train = sb.diverging_palette(220, 10, as_cmap=True)\n",
    "sb.heatmap(df_corr, mask=mask_train, cmap=cmap_train, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cover_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll look at skewness visually as well, but first we need to separate the continuous variables from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "cont_df = df[['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology','Horizontal_Distance_To_Roadways',\n",
    "              'Hillshade_9am','Hillshade_Noon','Hillshade_3pm','Horizontal_Distance_To_Fire_Points']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.set_style(\"darkgrid\")\n",
    "for i, col in enumerate(cont_df.columns):\n",
    "    plt.figure(i)\n",
    "    t = sb.distplot(cont_df[col],color=\"g\",label=\"Skewness: {0:.2f}\".format(cont_df[col].skew()))\n",
    "    t.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the next visuals we can see not just the quartiles and median but also the probability density of the data at different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cover_Type']=df['Cover_Type'].astype('category')\n",
    "\n",
    "for i, col in enumerate(cont_df.columns):\n",
    "    plt.figure(i,figsize=(8,4))\n",
    "    sb.violinplot(x=df['Cover_Type'], y=col, data=df, palette=\"mako\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns\n",
    "\n",
    "#number of rows=r, number of columns=c\n",
    "r,c = df.shape\n",
    "\n",
    "#Create a new dataframe with r rows, one column for each encoded category, and target\n",
    "newdf = pd.DataFrame(index=np.arange(0, r),columns=['Wilderness_Area','Soil_Type','Cover_Type'])\n",
    "\n",
    "#Make an entry in 'newdf' for each r as category_id, target value\n",
    "for x in range(0,r):\n",
    "    w=0;\n",
    "    s=0;\n",
    "    \n",
    "    # Category1\n",
    "    for y in range(10,14):\n",
    "        if (df.iloc[x,y] == 1):\n",
    "            w=y-9  #category class\n",
    "            break\n",
    "            \n",
    "    # Category2       \n",
    "    for z in range(14,54):\n",
    "        if (df.iloc[x,z] == 1):\n",
    "            s=z-13 #category class\n",
    "            break\n",
    "    #Make an entry in 'data' for each r as category_id, target value        \n",
    "    newdf.iloc[x]=[w,s,df.iloc[x,c-1]]\n",
    "\n",
    "#Category 1:\n",
    "sb.countplot(x=\"Wilderness_Area\", hue=\"Cover_Type\", data=newdf, palette=\"viridis\")\n",
    "plt.show()\n",
    "\n",
    "#Category 2:\n",
    "plt.rc(\"figure\", figsize=(25, 10))\n",
    "sb.countplot(x=\"Soil_Type\", hue=\"Cover_Type\", data=newdf, palette=\"Spectral\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at on the correlations between features again, now only with regards to continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "sb.heatmap(cont_df.corr(),cmap='viridis',annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sb.PairGrid(cont_df)\n",
    "g.map(sb.scatterplot, color=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the dataset to features and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.loc[:,'Elevation':'Soil_Type40']\n",
    "y=df['Cover_Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's remove the features with low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colstodrop=['Hillshade_3pm','Soil_Type7','Soil_Type8','Soil_Type14','Soil_Type15',\n",
    "     'Soil_Type21','Soil_Type25','Soil_Type28','Soil_Type36','Soil_Type37']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(colstodrop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.0001, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making pipeline including scaling, grid search and cross validation for each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipe = make_pipeline(StandardScaler(), SVC())  \n",
    "param_grid = {'svc__C': [1e3, 1e4, 1e5],\n",
    "              'svc__gamma': [0.001, 0.01, 0.1]}\n",
    "gs = GridSearchCV(pipe, param_grid, cv=5)\n",
    "gs.fit(X_train[:2000], y_train[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svc = make_pipeline(StandardScaler(),SVC(kernel='rbf',C=1000,gamma=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred, labels=['Spruce/Fir', 'Lodgepole Pine', 'Ponderosa Pine','Cottonwood/Willow','Aspen','Douglas-fir','Krummholz'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target_names = ['Spruce/Fir', 'Lodgepole Pine', 'Ponderosa Pine','Cottonwood/Willow','Aspen','Douglas-fir','Krummholz']\n",
    "classification_report(y_test, y_pred, target_names=target_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
